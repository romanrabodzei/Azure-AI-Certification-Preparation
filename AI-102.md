# Azure AI Engineer Associate

**This certification validates your expertise with Azure AI Engineer Associate.**
<p>
Candidates for this exam should be professionals with experience in designing and implementing AI solutions on Microsoft Azure. This exam is an opportunity to demonstrate proficiency in leveraging Azure AI services to create, manage, and secure AI applications, including integrating them with cognitive services, machine learning, and knowledge-mining tools.
</p>

> [!IMPORTANT]
> **These are NOT real questions from the exam, but quite close enough to what you can get. The goal is to help you prepare and obtain the certification.**

## Skills measured

- Plan and manage an Azure AI solution (15–20% of the exam)
- Implement content moderation solutions (10–15% of the exam)
- Implement computer vision solutions (15–20% of the exam)
- Implement natural language processing solutions (30–35% of the exam)
- Implement knowledge mining and document intelligence solutions (10–15% of the exam)
- Implement generative AI solutions (10–15% of the exam)

## Questions

Q: You have a website that allows users to upload images.
You need to ensure that the uploaded images do not contain adult content. The solution must minimize development effort.
Which service should you use?
<details><summary>Show the answer</summary><p>

- **Azure AI Vision Image Analysis**
> The Azure AI Vision Image Analysis service can extract a wide variety of visual features from an image. One of them is to detect adult content.
The Azure AI Face service provides AI algorithms that detect, recognize, and analyze human faces in images. Azure AI Custom Vision is an image recognition service that lets you build, deploy, and improve own image identifier models. So, while it is possible, it is not the solution with the lowest development effort. Azure AI Vision Spatial Analysis is used to ingest streaming video from cameras, extract insights, and generate events to be used by other systems. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/what-are-cognitive-services) [Source<sup>2</sup>](https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview) [Source<sup>3</sup>](https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-image-analysis?tabs=3-2) [Source<sup>4</sup>](https://learn.microsoft.com/training/modules/prepare-to-develop-ai-solutions-azure/)
</details>

---
Q: You are building a solution that uses Azure AI Search.
You need to execute the initial run of the indexer.
Which stages will be included during the initial run?
<details><summary>Show the answer</summary><p>

- **Document cracking, field mapping, skillset execution, and output field mapping**
> Document cracking, field mapping, skillset execution, and output field mapping are the stages of indexing.
Creating a data source, creating an index, and creating and running the indexer are the stages to create an indexer. Connecting to an Azure data source, creating an index schema, and running the wizard to create objects and load data are the stages for the Import Data wizard. [Source<sup>1</sup>](https://learn.microsoft.com/azure/search/search-indexer-overview#stages-of-indexing) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/)
</details>

---
Q: You are building a solution that uses Azure AI Search.
You need to save normalized binary files as projections.
Which type of projection should you use?
<details><summary>Show the answer</summary><p>

- **Files**
> Tables are used for data that is best represented as rows and columns, or whenever you need granular representations of your data.
Files are used when you need to save normalized, binary image files. Objects are used when you need the full JSON representation of your data and enrichments in one JSON document. [Source<sup>1</sup](https://learn.microsoft.com/azure/search/knowledge-store-projection-overview#types-of-projections-and-usage) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/)
</details>

---
Q: You are building a knowledge mining solution by using Azure AI Search.
You need to ensure that the solution supports wildcard queries in search requests.
What should you include in the REST API request?
<details><summary>Show the answer</summary><p>

- **“queryType”: “full”**
> queryType “full” extends the default Simple query language by adding support for more operators and query types, such as wildcard, fuzzy, regex, and field-scoped queries. [Source<sup>1</sup>](https://learn.microsoft.com/azure/search/search-lucene-query-architecture) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/)
</details>

---
Q: You are building an app will use Azure AI Search.
You need to index a collection of documents.
What is the first stage of the indexing process?
<details><summary>Show the answer</summary><p>

- **Document cracking**
> Document cracking is the process of opening files and extracting content. It is the first stage of the indexing process.
Text-based content can be extracted from files in a service, rows in a table, or items in a container or collection. If you add a skillset and image skills, document cracking can also extract images and queue them for image processing. [Source<sup>1</sup>](https://learn.microsoft.com/azure/search/search-indexer-overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/)
</details>

---
Q: You have a web app named App1 that performs customs searches.
You are building a solution that uses Azure AI Search.
You need to include App1 as a custom skill as part of the solution.
Which @odata.type should you use to call App1?
<details><summary>Show the answer</summary><p>

- **Microsoft.Skills.Custom.WebApiSkill**
> Microsoft.Skills.Custom.WebApiSkill allows the extensibility of an AI enrichment pipeline by making an HTTP call to a custom web API. [Source<sup>1</sup>](https://learn.microsoft.com/azure/search/cognitive-search-predefined-skills) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-enrichment-pipeline-azure-cognitive-search/)
</details>

---
Q: You are building a knowledge mining solution that uses Azure AI Search.
You need to extract content from a file within the enrichment pipeline by using AI enrichment.
Which built-in skill should you use?
<details><summary>Show the answer</summary><p>

- **Microsoft.Skills.Util.DocumentExtractionSkill**
> Microsoft.Skills.Util.DocumentExtractionSkill is the built-in skill used to extract content from a file within the enrichment pipeline. [Source<sup>1</sup>](https://learn.microsoft.com/azure/search/cognitive-search-predefined-skills) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-azure-cognitive-search-solution/)
</details>

---
Q: You have an app named App1 that analyzes social media mentions and determines whether comments are positive or negative.
During testing, you notice that App1 generates negative sentiment analysis in response to customer feedback that contains positive feedback.
You need to ensure that App1 includes more granular information during the analysis.
What should you add to the API requests?
<details><summary>Show the answer</summary><p>

- **opinionMining=true**
> opinionMining=true will add aspect-based sentiment analysis, which in turn will make the sentiment more granular so that positive and negative in a single sentence can be returned.
loggingOptOut=true will opt out of logging and StringIndexType=TextElements_v8 will set the returned offset and length values to correspond with TextElements. [Source<sup>1</sup>](https://learn.microsoft.com/rest/api/language/text-analysis-runtime/analyze-text?view=rest-language-2023-04-01&tabs=HTTP) [Source<sup>2</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/sentiment-opinion-mining/how-to/call-api) [Source<sup>3</sup>](https://learn.microsoft.com/training/modules/extract-insights-text-with-text-analytics-service/)
</details>

---
Q: You are building an app that will flag documents that contain the names of staff members by using the Azure AI Language Personally Identifiable Information (PII) detection feature.
You need to configure the PII detection feature.
Which category should you use?
<details><summary>Show the answer</summary><p>

- **Person**
> The Person category detects names of people in the PII detection feature. The PhoneNumber category detects phone numbers, the age category detects people’s ages, and the DateTime detects dates and time values. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/personally-identifiable-information/concepts/entity-categories) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/publish-use-language-understand-app/)
</details>

---
Q: You need to build an app that will summarize text documents into key phrases.
Which Azure AI Language feature should you recommend?
<details><summary>Show the answer</summary><p>

- **Key phrase extraction**
> Key phrase extraction is used to quickly identify the main concepts in text, whereas the other features do not return key phrases from longer text documents. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/key-phrase-extraction/overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/extract-insights-text-with-text-analytics-service/)
</details>

---
Q: You have an app that sends audio recordings from a call center to the speech-to-text feature of Azure AI Services.
During testing, you notice that the Word Error Rate (WER) is high and there are a lot of substitution errors.
You need to improve the model and reduce the WER.
What should you add to the training data?
<details><summary>Show the answer</summary><p>

- **Custom product and people names**
> Substitution errors are due to the model needing more training on custom product names and people names.
Overlapping speakers define when there are more deletion errors. People talking in the background are detected when there are more insertion errors. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/speech-service/how-to-custom-speech-evaluate-data?pivots=speech-studio#resolve-errors-and-improve-wer) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/transcribe-speech-input-text/)
</details>

---
Q: You are building an app that will analyze meeting recordings and identify who is speaking at which moment in time.
You need to configure a voice profile for the app.
Which type of voice profile should you use?
<details><summary>Show the answer</summary><p>

- **Speaker identification**
> Text-independent verification means that speakers can speak in everyday language in enrollment and verification phases.
Text-dependent verification means that speakers need to choose the same passphrase to use during both enrollment and verification phases. This is the voice profile that should be used when configuring a voice profile for the app.
Speaker identification helps you determine an unknown speaker’s identity within a group of enrolled speakers. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/speech-service/speaker-recognition-overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/transcribe-speech-input-text/)
</details>

---
Q: You are building an app that will enable users to create notes by using speech.
You need to recommend the Azure AI Speech service model to use. The solution must support noisy environments.
Which model should you recommend?
<details><summary>Show the answer</summary><p>

- **Custom speech-to-text**
> The custom speech-to-text model is correct, as you need to adapt the model because a factory floor might have ambient noise, which the model should be trained on. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/speech-service/faq-stt) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/transcribe-speech-input-text/)
</details>

---
Q: You plan to build an app that will transcribe large quantities of audio files by using the Azure AI Speech service batch transcription feature.
You need to recommend a storage solution for the audio files. The solution must minimize development effort.
What should you recommend?
<details><summary>Show the answer</summary><p>

- **Azure Storage**
> Azure Storage is the only storage provider that can be used by default for batch transcription. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/speech-service/batch-transcription-create?pivots=speech-cli) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/transcribe-speech-input-text/)
</details>

---
Q: You are building an app that will recognize the intent and entities of user utterances in real-time.
You need to implement the pattern matching intent recognition mechanism. The solution must only detect entities that you define in a catalog of phrases.
Which entity type should you use?
<details><summary>Show the answer</summary><p>

- **The List entity using Strict mode**
> The List entity is made up of a list of phrases that will guide the engine on how to match the text. When an entity has an ID of type List and is in Strict mode, the engine will only match if the text in the slot appears in the list. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/speech-service/pattern-matching-overview#types-of-entities) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/transcribe-speech-input-text/)
</details>

---
Q: You are building a custom translation model.
You need to use bilingual training documents to teach the model your terminology and style.
Which rule should you follow?
<details><summary>Show the answer</summary><p>

- **Be liberal**
> Be liberal is correct. Any in-domain human translation is better than machine translation. Add and remove documents as you go and try to improve the Bilingual Evaluation Understudy (BLEU) score.
Be strict is incorrect. Compose them to be optimally representative of what you are going to translate in the future. Be restrictive is also incorrect. A phrase dictionary is case-sensitive, and any word or phrase listed is translated in the way you specify. In many cases, it is better to not use a phrase dictionary and let the system learn. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/translator/custom-translator/beginners-guide#what-should-i-use-for-training-material) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/translate-text-with-translator-service/)
</details>

---
Q: You are building a custom translation model.
You need to evaluate the precision of the text that you translated by using a Bilingual Evaluation Understudy (BLEU) score.
Which scale is used for the score?
<details><summary>Show the answer</summary><p>

- **Between 0 and 100**
> A BLEU score is a number between zero and 100. A score of zero indicates a low-quality translation, where nothing in the translation matches the reference. A score of 100 indicates a perfect translation that is identical to the reference. It is unnecessary to attain a score of 100. A BLEU score between 40 and 60 indicates a high-quality translation. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/translator/custom-translator/beginners-guide#what-is-a-bleu-score) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/translate-text-with-translator-service/)
</details>

---
Q: You are using Azure AI Translator to translate documents from one language to another.
You need to extend the capabilities of an application by using the Azure AI Translator service.
Which three features are available in the Translator service? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **Detect language**
- **Dictionary lookup**
- **Transliterate**
> Apart from translation, the following features are part of Translator: Transliterate, Detect, Dictionary lookup, and Dictionary example. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/translator/text-translation-overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/translate-text-with-translator-service/)
</details>

---
Q: You are building an Azure AI Translator custom model.
You need to ensure that the translation accuracy for the model has a Bilingual Evaluation Understudy (BLEU) score that indicates high quality.
What is the minimum score range required?
<details><summary>Show the answer</summary><p>

- **40 to 59**
> between 40 and 60 indicates a high-quality translation. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/translator/custom-translator/key-terms) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/translate-text-with-translator-service/)
</details>

---
Q: You are building a model that uses Conversational Language Understanding (CLU).
You need to train the model.
Which training methods can you use?
<details><summary>Show the answer</summary><p>

- **Standard and advanced only**
> Both standard and advanced are from CLU. Deterministic is a method from Language Understanding. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/conversational-language-understanding/how-to/train-model?tabs=language-studio#training-modes) [Source<sup>2</sup>](https://learn.microsoft.com/azure/cognitive-services/luis/how-to/train-test#change-deterministic-training-settings-using-the-version-settings-api) [Source<sup>3</sup>](https://learn.microsoft.com/training/modules/build-language-understanding-model/)
</details>

---
Q: You are creating an orchestration workflow for Language Understanding.
You need to configure workflows for multiple languages. The solution must minimize administrative effort.
What should you create for each language?
<details><summary>Show the answer</summary><p>

- **Separate workflow projects**
> Orchestration workflow projects do not support the multilingual option, so you need to create a separate workflow project for each language. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/orchestration-workflow/language-support) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/build-language-understanding-model/)
</details>

---
Q: You are building an orchestration workflow to orchestrate and connect multiple Language Understanding models and question answering projects for use in a bot.
Which two operations can you perform in an orchestration workflow based on Azure AI Language service? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **Connect to Language Understanding applications that are owned by the same resource as the orchestration workflow**
- **Connect to question answering projects that are in the same Azure AI Language service resource as your orchestration workflow**
> Adding entities to your orchestration workflow is not allowed. Entities are mutually exclusive within an orchestration workflow. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/orchestration-workflow/faq) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/build-language-understanding-model/)
</details>

---
Q: You plan to build a chatbot that will help users answer FAQs.
You need to identify which scenarios are suitable for use with the Azure AI Language question answering service.
Which three scenarios should you identify? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **When you have a bot conversation that includes static information**
- **When you have static information in a knowledge base of answers**
- **When you need to provide the same answer to a request, question, or command**
> Question answering only works with static information, not with dynamic information. In addition, it will always provide the same answer to the same question. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/)
</details>

---
Q: You are building a chatbot that will use the Azure AI Language question answering service.
You need to identify the operational costs for the service.
Which two parameters will influence the costs? Each correct answer presents a complete solution
<details><summary>Show the answer</summary><p>

- **The required throughput**
- **The size and the number of knowledge bases**
> The throughput, the size, and the number of knowledge bases affect the pricing tier, whereas the other parameters do not affect pricing. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/language-service/question-answering/concepts/azure-resources) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/build-qna-solution-qna-maker/)
</details>

---
Q: You are building an app that will detect the color scheme of uploaded images.
You are evaluating using the Image Analysis API to detect the dominant background color of an image.
Which color can the API return as a dominant background color?
<details><summary>Show the answer</summary><p>

- **Teal**
> Only a certain set of colors can be returned by the API. The set of possible returned colors is black, blue, brown, gray, green, orange, pink, purple, red, teal, white, and yellow. [Source](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-detecting-color-schemes)
</details>

---
Q: You are building an app that will use Azure AI Vision to detect the presence of people in a video feed.
Which Azure AI Vision feature should you use?
<details><summary>Show the answer</summary><p>

- **Spatial Analysis**
> The only visual feature that provides this capability is Spatial Analysis, as OCR, Image Analysis, and face detection are not meant to analyze the presence of people in a video feed. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/analyze-video/)
</details>

---
Q: You have an app named App1 that extracts invoice data from PDF files by using an S0 instance of Azure AI Document Intelligence. The PDF files are up to 2 MB each and contain up to 10 pages.
Users report that App1 is unable to process some invoices.
You need to troubleshoot the issue.
What is a possible cause of the issue?
<details><summary>Show the answer</summary><p>

- **Some of the files are password protected.**
> The service cannot process password-protected files, and this can cause the service a processing failure for some of the files. Although file size and number of pages can cause failures, the limit for the S0 tier is 500 MB and 2,000 pages.
The S0 tier is sufficient for the file characteristics mentioned. [Source<sup>1</sup>](https://learn.microsoft.com/azure/applied-ai-services/form-recognizer/concept-invoice?view=form-recog-3.0.0) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/work-form-recognizer/)
</details>

---
Q: You are building an app that will use the Azure AI Custom Vision API to detect when all the spaces in a parking lot are empty.
Which feature of the API should you use?
<details><summary>Show the answer</summary><p>

- **Object detection**
> Object detection is similar to image classification, but it returns the coordinates in an image where the applied label(s) can be found.
Image description analyzes an image and generates a human-readable phrase that describes its contents. Image classification applies one or more labels to an entire image. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/detect-objects-images/)
</details>

---
Q: You need to build an app that will use Azure AI Vision to analyze and detect animals in images.
Which type of project should you use?
<details><summary>Show the answer</summary><p>

- **Object detection**
> Object detection returns the coordinates in an image where the applied label(s) can be found, while image classification applies one or more labels to an entire image. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/custom-vision-service/overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/detect-objects-images/)
</details>

---
Q: You are building an app that uses Azure AI Video Indexer.
You need to extract keyframes from uploaded video and store them on a disk by using the API.
How should you implement the solution?
<details><summary>Show the answer</summary><p>

- **Upload the video, get the video index, and get the thumbnail for each keyframe.**
> You need to upload the video, get the video index, and get the thumbnail for each keyframe. Three API calls need to be done.
Uploading the video, and then downloading the ZIP file of the thumbnails is the path through the Azure portal. You need the index to know the correct parameters for the thumbnail request. [Source<sup>1</sup>](https://learn.microsoft.com/azure/azure-video-indexer/scenes-shots-keyframes) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/analyze-video/)
</details>

---
Q: You are using a custom Language content model in an Azure AI Video Indexer solution.
During testing, you upload a text file that includes the following sentence: “Kubernetes is a new feature in Azure & the cloud.”
The sentence is discarded.
You need to ensure that the model retains the sentence.
What should you do?
<details><summary>Show the answer</summary><p>

- **Remove the “&” character from the text file**
> You need to remove the “&” character because sentences with special characters will be discarded.
Kubernetes is highly specific and unknown to the model, so retraining the model is incorrect. The slate model is for clapper boards and digital patterns with color bars. [Source<sup>1</sup>](https://learn.microsoft.com/azure/azure-video-indexer/customize-language-model-overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/analyze-video/)
</details>

---
Q: You are building a video processing app that will use Azure AI Video Indexer.
You need to configure the training and learning phases for the app. The solution must train the model based on the probability of specific word combinations by using a custom Language model.
Which three practices should be followed for the training data? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **Include multiple examples of spoken sentences.**
- **Provide multiple adaptation options.**
- **Put only one sentence per line.**
> When training the model, you should avoid repeating an identical sentence multiple times, as it may create bias against the rest of the input.
You should avoid including uncommon symbols (~, # @ % &), as they will be discarded. The sentences in which they appear will also be discarded.
You should also avoid putting inputs that are too large, such as hundreds of thousands of sentences, because doing so will dilute the effect of boosting. [Source<sup>1</sup>](https://learn.microsoft.com/azure/azure-video-indexer/customize-language-model-overview) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/analyze-video/)
</details>

---
Q: You are building a video processing app that will use Azure AI Video Indexer to extract insights from videos that contain multi-language content.
You need to configure the API calls to enable multilingual identification.
Which value should you set for the sourceLanguage parameter?
<details><summary>Show the answer</summary><p>

- **Multi-language detection**
> When indexing or reindexing a video by using the API, choose the multi-language detection option for the sourceLanguage parameter. The remaining options do not configure the API calls to enable multilingual identification. [Source<sup>1</sup>](https://learn.microsoft.com/azure/azure-video-indexer/multi-language-identification-transcription) [Source<sup>2</sup>](https://learn.microsoft.com/azure/azure-video-indexer/multi-language-identification-transcription) [Source<sup>3</sup>](https://learn.microsoft.com/training/modules/analyze-video/)
</details>

---
Q: You plan to implement a solution to extract information from documents by using Azure AI Document Intelligence and use prebuilt models where possible.
You need to identify the prebuilt models available in Azure AI Document Intelligence.
Which three models are available? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **Invoice**
- **Receipt**
- **W-2**
> Except for meeting minutes and time sheets, all the other models are prebuilt models in Azure AI Document Intelligence. Meeting minutes and time sheets must be added to Azure AI Document Intelligence. as custom models. [Source](https://learn.microsoft.com/training/modules/use-prebuilt-form-recognizer-models/4-use-financial-id-tax-models)
</details>

---
Q: You are building a mobile app that will enable users to scan street signs and will read out the text on the sign.
You need to recommend a service to use. The solution must minimize development effort.
Which service should you recommend?
<details><summary>Show the answer</summary><p>

- **Azure AI Vision**
> Azure AI Vision is the only service which can achieve the desired result.
Azure AI Custom Vision and Azure AI Face do not offer OCR. Azure AI Document Intelligence is designed for documents, but not images. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-ocr) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/read-text-images-documents-with-computer-vision-service/)
</details>

---
Q: You are building an app that will extract text from scanned receipts.
You need to recommend which service to use. The solution must minimize development effort.
What should you recommend?
<details><summary>Show the answer</summary><p>

- **Azure AI Document Intelligence**
> Azure AI Document Intelligence is designed to work with documents such as receipts, as it offers prebuilt models for extracting information from these kinds of documents. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/computer-vision/overview-ocr) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/work-form-recognizer/)
</details>

---
Q: You are building an Azure AI Language solution.
You need to deploy the solution to a location without internet connectivity.
What should you do?
<details><summary>Show the answer</summary><p>

- **Deploy the solution to a Docker host container.**
> You can use disconnected containers to host Language Understanding on-premises.
There is no virtual machine set up to run a Language Understanding instance and a standard instance still has Language Understanding running in the cloud. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/containers/disconnected-containers) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/investigate-container-for-use-cognitive-services/)
</details>

---
Q: You are building an app that will use an Azure AI Services resource.
You need to identify the endpoint for the resource.
From the Azure CLI, which command should you run?
<details><summary>Show the answer</summary><p>

- **az cognitiveservices account show --name myresource --resource-group cognitive-services-resource-group**
> As you need to provide the name and the resource group for your Cognitive Service account to retrieve the endpoint amongst other information for the resource, az cognitiveservices account show --name myresource --resource-group cognitive-services-resource-group is the only valid command. [Source<sup>1</sup>](https://learn.microsoft.com/cli/azure/cognitiveservices/account?view=azure-cli-latest#az-cognitiveservices-account-show) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/create-manage-cognitive-services/)
</details>

---
Q: You are building an app that will use Azure AI Custom Vision. The app will be deployed to a virtual machine in Azure.
You enable firewall rules for your Azure AI Services account.
You need to ensure that the app can access the service through a service endpoint.
What should you do?
<details><summary>Show the answer</summary><p>

- **Grant access to a specific virtual network.**
> If you enable the firewall for the Azure AI Services account, you need to allow network access to the service. You can achieve this by either allowing access from a specific virtual network or adding an IP range to the firewall rules. In this situation, the app is deployed to a virtual machine in Azure, which resides in a virtual network. You can provide access to virtual networks in Azure to access specific service endpoints. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/cognitive-services-virtual-networks?context=%2Fazure%2Fcognitive-services%2Fcustom-vision-service%2Fcontext%2Fcontext&tabs=portal) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/secure-cognitive-services/)
</details>

---
Q: You have an Azure App Services web app named App1.
You need to configure App1 to use Azure AI Services to authenticate by using Microsoft Entra ID. The solution must meet the following requirements:
Minimize administrative effort.
Use the principle of least privilege.
What should you do?
<details><summary>Show the answer</summary><p>

- **From App1, enable a managed identity and assign role-based access control (RBAC) permissions to Azure AI Services.**
> With a managed identity, the rotation of the secrets (certificates) is done automatically.
You still need to rotate secrets by using the key vault, and you cannot create secrets that never expire from the portal. It is not considered best practice to create one with PS1 or the CLI, and a certificate will also expire at some point. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/authentication?tabs=powershell#authorize-access-to-managed-identities) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/secure-cognitive-services/)
</details>

---
Q: You have an Azure AI Services resource.
You need to enable diagnostic logging.
What are two prerequisites for diagnostic logging? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **A Log Analytics workspace**
- **An Azure Storage account**
> The prerequisites to enable diagnostic logging are to have an Azure Storage resource that retains diagnostic logs for policy audit, static analysis, or backup. A Log Analytics resource provides a flexible log search and analytics tool that allows for analysis of raw logs generated by an Azure resource. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/diagnostic-logging) [Sourc<sup>2</sup>](https://learn.microsoft.com/training/modules/monitor-cognitive-services/)
</details>

---
Q: You are developing a containerized optical character recognition (OCR)-capable application by using Azure AI Services containers.
While developing the solution, you retrieve a status message of “Mismatch”, and the connection to the AI Services resource fails.
You need to ensure that the solution can connect to the AI Services resource.
What should you do?
<details><summary>Show the answer</summary><p>

- **Confirm that the API key is for the correct resource type.**
> Mismatch means that the wrong API key has been used. If you have provided an API key or endpoint for a different kind of Azure AI Services resource, you find your API key and service region in the Azure portal, in the Keys and Endpoint section for your Azure AI Services resource.
If the API key is invalid, you must confirm that the API key is for the correct region. If the API key has exceeded the quota, then you can either upgrade your pricing tier or wait for an additional quota to become available. Find your tier in the Azure portal, in the Pricing Tier section of your Azure AI Service resource. The mismatch error would not be generated if the resource was offline. [Source<sup>1</sup>](https://learn.microsoft.com/azure/cognitive-services/containers/container-faq) [Source<sup>2</sup>](https://learn.microsoft.com/training/modules/investigate-container-for-use-cognitive-services/)
</details>

---
Q: You have an Azure OpenAI solution. The solution uses a specific GPT-35-Turbo model version that was current during initial deployment. Auto-update is disabled.
Sometime later, you investigate the deployed solution and discover that it uses a newer version of the model.
Why was the model version updated?
<details><summary>Show the answer</summary><p>

- **The model version reached its retirement date.**
> As your use of Azure OpenAI evolves, and you start to build and integrate with applications, you might want to manually control model updates so that you can first test and validate whether model performance remains consistent for a use case before performing an upgrade.
When you select a specific model version for a deployment, this version will remain selected until you either choose to manually update it, or once you reach the retirement date of the model. When the retirement date is reached, the model will upgrade to the default version automatically at the time of retirement. [Source](https://learn.microsoft.com/azure/ai-services/openai/how-to/working-with-models?tabs=powershell)
</details>

---
Q: You are creating an application that references the Azure OpenAI REST API for a DALL-E model.
You plan to use thumbnails of the images that DALL-E generates and display them in a table on a webpage.
You need to find the image URLs in the JSON response.
Which element should you review?
<details><summary>Show the answer</summary><p>

- **The result element**
> The result from the initial request does not immediately return the results of the image generation process. Instead, the response includes an operation-location header with a URL for a callback service that your application code can poll until the results of the image generation are ready. The result element includes a collection of url elements, each of which references a PNG image file generated from the prompt. [Source](https://learn.microsoft.com/training/modules/generate-images-azure-openai/4-dall-e-rest-api)
</details>

---
Q: You are building a web app that will generate images based on user prompts. The app will use the DALL-E 3 Azure OpenAI model.
You need to ensure that HTTP requests against the Azure OpenAI API successfully generate images.
Which three HTTP header properties should you include? Each correct answer presents part of the solution.
<details><summary>Show the answer</summary><p>

- **The API version used in this operation**
- **The name of the Azure OpenAI service resource**
- **The name of the Azure OpenAI service resource**
> The name of the Azure OpenAI resource, the name of the DALL-E 3 model deployment, and the API version to be used are the three required header properties for HTTP requests. The other answers are valid for use in the HTTP body but not the header. [Source](https://learn.microsoft.com/azure/ai-services/openai/reference)
</details>

---
Q: You are deploying an Azure OpenAI service.
You plan to use your own data in the models you will deploy.
You need to ensure that the model can index your data sources.
Which additional Azure service should you deploy?
<details><summary>Show the answer</summary><p>

- **Azure AI Search**
> Azure OpenAI on your data enables developers to use supported AI chat models that can reference specific sources of information to ground the response. Adding this information allows the model to reference both the specific data provided and its pretrained knowledge to provide more effective responses. Azure OpenAI on your data utilizes the search ability of Azure AI Search to add the relevant data chunks to a prompt.
Azure OpenAI on your data still uses a stateless API to connect to the model, which removes the requirement of training a custom model with your data and simplifies the interaction with the AI model. Cognitive Search first finds the useful information to answer the prompt, and Azure OpenAI forms the response based on that information. [Source<sup>1</sup>](https://learn.microsoft.com/training/modules/use-own-data-azure-openai/2-understand-use-own-data) [Source<sup>2</sup>](https://learn.microsoft.com/azure/ai-services/what-are-ai-services)
</details>

---
Q: You are building a GPT-based chat application that will answer questions about your company.
You plan to use the Using your data feature in Azure OpenAI to ground the model with company data.
Which four types of files can you use to ground the model? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **HTML**
- **MD**
- **PDF**
- **TXT**
> Currently only TXT, MD, HTML, PDF, Microsoft Word, and PowerPoint files can be used and are supported using the “Using your data” feature in Azure OpenAI. ZIP and XML files are not supported. [Source](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search)
</details>

---
Q: You are building a GPT-based chat application that will answer questions about your company.
You plan to use the Using your data feature in Azure OpenAI to ground the model with your company data.
While testing, you discover that some responses are not accurate enough.
You need to configure the Azure OpenAI resource to filter out less-relevant documents for responses.
Which parameter should you configure?
<details><summary>Show the answer</summary><p>

- **Strictness**
> The Strictness parameter sets the threshold to categorize documents as relevant to your queries. Raising the Strictness parameter value means a higher threshold for relevance and filters out more less-relevant documents for responses. Retrieved documents specifies the number of top-scoring documents from your data index used to generate responses. Content data specifies the fields in your index that contain the main text content of each document. File name specifies the field in your index that contains the original file name of each document. [Source](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search)
</details>

---
Q: You are building a GPT-based chat application that will answer questions about your company.
You plan to test the application by using strategies defined by Microsoft best practices.
Which three prompt engineering strategies should you consider while testing the application? Each correct answer presents a complete solution.
<details><summary>Show the answer</summary><p>

- **Be Descriptive**
- **Be Specific**
- **Order Matters**
> Be Specific means to leave as little to interpretation as possible. Be Descriptive means to use analogies. Order Matters means that the order in which you present information to the model can affect the output. Therefore, those three are valid best practices. Be simple and be minimalistic do not produce the best results and are, therefore, not best practices. [Source](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering)
</details>

---
Q: You are creating an application that will use Azure OpenAI REST API services. The application uses a REST call to a DALL-E model to generate images. The three parameters in the REST call are prompt, n, and size.
What does the size parameter indicate?
<details><summary>Show the answer</summary><p>

- **The size of the images in pixel resolution**
> To make a REST call to the services, you need the endpoint and authorization key for the Azure OpenAI service resource you provisioned in Azure. You initiate the image generation process by submitting a POST request to the service endpoint that has the authorization key in the header. The request must contain the following parameters in a JSON body:
`prompt`: The description of the image to be generated
`n`: The number of images to be generated
`size`: The resolution of the image to be generated (256x256, 512x512, or 1024x1024)
[Source](https://learn.microsoft.com/training/modules/generate-images-azure-openai/4-dall-e-rest-api)
</details>

---
